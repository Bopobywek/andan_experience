{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUAWw716dcY8"
      },
      "source": [
        "# HSE 2021: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 4\n",
        "\n",
        "**Warning 1**: You have 2 weeks for this assignemnt.  **it is better to start early (!)**\n",
        "\n",
        "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
        "\n",
        "\n",
        "### Contents\n",
        "\n",
        "#### Decision Trees - 7 points\n",
        "* [Task 1](#task1) (0.5 points)\n",
        "* [Task 2](#task2) (0.5 points)\n",
        "* [Task 3](#task3) (2 points)\n",
        "* [Task 4](#task4) (0.5 points)\n",
        "* [Task 5](#task5) (0.5 points)\n",
        "* [Task 6](#task6) (2 points)\n",
        "* [Task 7](#task7) (0.5 points)\n",
        "* [Task 8](#task8) (0.5 points)\n",
        "\n",
        "#### Ensembles - 3 points\n",
        "* [Task 1](#task2_1) (1 point)\n",
        "* [Task 2](#task2_2) (0.7 points)\n",
        "* [Task 3](#task2_3) (0.5 points)\n",
        "* [Task 4](#task2_4) (0.7 points)\n",
        "* [Task 5](#task2_5) (0.1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.727636Z",
          "start_time": "2023-11-09T17:46:40.045013Z"
        },
        "id": "g3t0v2bHdcZX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (11, 5)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhg34DWQdcZd"
      },
      "source": [
        "# Part 1. Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyphjR-NdcZi"
      },
      "source": [
        "In this task you will be implementing decision tree for the regression by hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwfM8O9vdcZl"
      },
      "source": [
        "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
        "\n",
        "Here you should implement the function `H()` which calculates impurity criterion. We will be training regression tree, and will take mean absolute deviation as impurity criterion.\n",
        "\n",
        "* You cannot use loops\n",
        "* If `y` is empty, the function should return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.732847Z",
          "start_time": "2023-11-09T17:46:40.729241Z"
        },
        "id": "QVaVsRARdcZt"
      },
      "outputs": [],
      "source": [
        "def H(y):\n",
        "    \"\"\"\n",
        "    Calculate impurity criterion\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.array\n",
        "        array of objects target values in the node\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    H(R) : float\n",
        "        Impurity in the node (measuread by variance)\n",
        "    \"\"\"\n",
        "    if y.size == 0:\n",
        "        return 0\n",
        "\n",
        "    # https://www.google.com/search?q=mean+absolute+deviation+formula&si=ALGXSlak8K6umOBV_MCwx-aeRh6TYd9RdMu4ntu98_aLkwrWoYxNYLA3nEbi2NNy024Eu-d1h11N3a7HoP2_nwEyJMR0W6532y3T1eLPRt0JWlMCRHKloURuR3in980Hj9cxnwrF6FYAh_lWYofS6mQKgdf5hDRsmA%3D%3D&ictx=1&ved=2ahUKEwj3qcqMjc2CAxWeRPEDHSmnAW4Qw_oBegQIRRAC\n",
        "    return np.mean(np.abs(y - np.mean(y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.875430Z",
          "start_time": "2023-11-09T17:46:40.737694Z"
        },
        "id": "883xLAR8dcZy"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)\n",
        "assert np.allclose(H(np.array([])), 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JNYsPcHdcZ3"
      },
      "source": [
        "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
        "\n",
        "To find the best split in the node we need to calculate the cost function. Denote:\n",
        "- `R` all the object in the node\n",
        "- `j` index of the feature selected for the split\n",
        "- `t` threshold\n",
        "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
        "\n",
        "We get the following cost function:\n",
        "\n",
        "$$\n",
        "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
        "$$\n",
        "\n",
        "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.877419Z",
          "start_time": "2023-11-09T17:46:40.877407Z"
        },
        "id": "0ibjfRT1dcZ8"
      },
      "outputs": [],
      "source": [
        "def Q(X, y, j, t):\n",
        "    \"\"\"\n",
        "    Calculate cost function\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray\n",
        "        array of objects in the node\n",
        "    y : ndarray\n",
        "        array of target values in the node\n",
        "    j : int\n",
        "        feature index (column in X)\n",
        "    t : float\n",
        "        threshold\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Q : float\n",
        "        Value of the cost function\n",
        "    \"\"\"\n",
        "    X_f = X[:, j]\n",
        "    y_left = y[X_f < t]\n",
        "    y_right = y[X_f >= t]\n",
        "\n",
        "    return y_left.size / y.size * H(y_left) + y_right.size / y.size * H(y_right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_67BH9XKdcaC"
      },
      "source": [
        "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
        "\n",
        "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
        "\n",
        "- `best_split`\n",
        "- `grow_tree`\n",
        "- `get_prediction`\n",
        "\n",
        "Also, please add `min_samples_leaf` parameter to your class\n",
        "\n",
        "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.878647Z",
          "start_time": "2023-11-09T17:46:40.878635Z"
        },
        "id": "pQnlX1eGdcaE"
      },
      "outputs": [],
      "source": [
        "class Node(object):\n",
        "    \"\"\"\n",
        "    Class for a decision tree node.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    right : Node() or None\n",
        "        Right child\n",
        "    right : Node() or None\n",
        "        Left child\n",
        "    threshold: float\n",
        "\n",
        "    column: int\n",
        "\n",
        "    depth: int\n",
        "\n",
        "    prediction: float\n",
        "        prediction of the target value in the node\n",
        "        (average values calculated on a train dataset)\n",
        "    is_terminal:bool\n",
        "        indicates whether it is a terminal node (leaf) or not\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.right = None\n",
        "        self.left = None\n",
        "        self.threshold = None\n",
        "        self.column = None\n",
        "        self.depth = None\n",
        "        self.is_terminal = False\n",
        "        self.prediction = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        if self.is_terminal:\n",
        "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
        "        else:\n",
        "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \\\n",
        "            format(self.column, self.threshold, self.prediction)\n",
        "        return node_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.879812Z",
          "start_time": "2023-11-09T17:46:40.879801Z"
        },
        "id": "BjFG8orhdcaM"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "\n",
        "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
        "    \"\"\"\n",
        "    Class for a Decision Tree Regressor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "        Max depth of a decision tree.\n",
        "    min_samples_split : int\n",
        "        Minimal number of samples (objects) in a node to make a split.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best split in terms of Q of data in a given decision tree node.\n",
        "        Try all features and thresholds.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_objects, n_features)\n",
        "            Objects in the parent node\n",
        "        y : ndarray, shape (n_objects, )\n",
        "            1D array with the object labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        best_split_column : int\n",
        "            Index of the best split column\n",
        "        best_threshold : float\n",
        "            The best split condition.\n",
        "        X_left : ndarray, shape (n_objects_l, n_features)\n",
        "            Objects in the left child\n",
        "        y_left : ndarray, shape (n_objects_l, )\n",
        "            Objects labels in the left child.\n",
        "        X_right : ndarray, shape (n_objects_r, n_features)\n",
        "            Objects in the right child\n",
        "        y_right : ndarray, shape (n_objects_r, )\n",
        "            Objects labels in the right child.\n",
        "        \"\"\"\n",
        "\n",
        "        # To store best split parameters\n",
        "        best_split_column = None\n",
        "        best_threshold = None\n",
        "        # without splitting\n",
        "        best_cost = H(y)\n",
        "\n",
        "        for j in range(X.shape[1]):\n",
        "          X_f = X[:, j]\n",
        "          for t in np.unique(X_f):\n",
        "              val = Q(X, y, j, t)\n",
        "              y_right = y[X_f >= val]\n",
        "              y_left = y[X_f < val]\n",
        "              if (val < best_cost and y_right.size >= self.min_samples_leaf\n",
        "                  and y_left.size >= self.min_samples_leaf):\n",
        "                best_cost = val\n",
        "                best_split_column = j\n",
        "                best_threshold = t\n",
        "\n",
        "        if best_split_column is not None:\n",
        "            X_f = X[:, best_split_column]\n",
        "            X_left = X[X_f < best_threshold]\n",
        "            y_left = y[X_f < best_threshold]\n",
        "            X_right = X[X_f >= best_threshold]\n",
        "            y_right = y[X_f >= best_threshold]\n",
        "\n",
        "            self.split_columns_[best_split_column] = self.split_columns_.get(best_split_column, []) + [best_threshold]\n",
        "            return best_split_column, best_threshold, X_left, y_left, X_right, y_right\n",
        "        return [None] * 6\n",
        "\n",
        "    def is_terminal(self, node, y):\n",
        "        \"\"\"\n",
        "        Check terminality conditions based on `max_depth`,\n",
        "        `min_samples_split` parameters for a given node.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node,\n",
        "\n",
        "        y : ndarray, shape (n_objects, )\n",
        "            Object labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Is_termial : bool\n",
        "            If True, node is terminal\n",
        "        \"\"\"\n",
        "        if node.depth >= self.max_depth:\n",
        "            return True\n",
        "        if len(y) < self.min_samples_split:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def grow_tree(self, node, X, y):\n",
        "        \"\"\"\n",
        "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
        "         - check terminality conditions\n",
        "         - find best split if node is not terminal\n",
        "         - add child nodes to the node\n",
        "         - call the function recursively for the added child nodes\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node() object\n",
        "            Current node of the decision tree.\n",
        "        X : ndarray, shape (n_objects, n_features)\n",
        "            Objects\n",
        "        y : ndarray, shape (n_objects)\n",
        "            Labels\n",
        "        \"\"\"\n",
        "\n",
        "        if self.is_terminal(node, y):\n",
        "            node.is_terminal = True\n",
        "            return\n",
        "\n",
        "        best_split_column, best_threshold, X_left, y_left, X_right, y_right = self.best_split(X, y)\n",
        "        if best_split_column is None:\n",
        "            node.is_terminal = True\n",
        "            return\n",
        "\n",
        "        node.left = Node()\n",
        "        node.right = Node()\n",
        "        node.left.depth = node.depth + 1\n",
        "        node.right.depth = node.depth + 1\n",
        "        node.left.prediction = np.mean(y_left)\n",
        "        node.right.prediction = np.mean(y_right)\n",
        "        node.threshold = best_threshold\n",
        "        node.column = best_split_column\n",
        "\n",
        "        self.grow_tree(node.left, X_left, y_left)\n",
        "        self.grow_tree(node.right, X_right, y_right)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the Decision Tree Regressor.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
        "            The target values.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        self.split_columns_ = {}\n",
        "        X, y = check_X_y(X, y, accept_sparse=False)\n",
        "        self.is_fitted_ = True\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        # Initialize the tree (root node)\n",
        "        self.tree_ = Node()\n",
        "        self.tree_.depth = 1\n",
        "        self.tree_.prediction = np.mean(y)\n",
        "\n",
        "        # Grow the tree\n",
        "        self.grow_tree(self.tree_, X, y)\n",
        "        return self\n",
        "\n",
        "    def get_prediction(self, node, x):\n",
        "        \"\"\"\n",
        "        Get prediction for an object `x`\n",
        "            - Return prediction of the `node` if it is terminal\n",
        "            - Otherwise, recursively call the function to get\n",
        "            predictions of the proper child\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node() object\n",
        "            Current node of the decision tree.\n",
        "        x : ndarray, shape (n_features,)\n",
        "            Array of feature values of one object.\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : float\n",
        "            Prediction for an object x\n",
        "        \"\"\"\n",
        "        if node.is_terminal:\n",
        "            return node.prediction\n",
        "\n",
        "        if (x[node.column] < node.threshold):\n",
        "            return self.get_prediction(node.left, x)\n",
        "        return self.get_prediction(node.right, x)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Get prediction for each object in X\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        Returns\n",
        "        -------\n",
        "        y : ndarray, shape (n_samples,)\n",
        "            Returns predictions.\n",
        "        \"\"\"\n",
        "        # Check input and that `fit` had been called\n",
        "        X = check_array(X, accept_sparse=False)\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "\n",
        "        # Get predictions\n",
        "        y_predicted = []\n",
        "        for x in X:\n",
        "            y_curr = self.get_prediction(self.tree_, x)\n",
        "            y_predicted.append(y_curr)\n",
        "        return np.array(y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.880754Z",
          "start_time": "2023-11-09T17:46:40.880743Z"
        },
        "id": "q4wCb7UmdcaO"
      },
      "outputs": [],
      "source": [
        "# check yourself\n",
        "from sklearn.utils.estimator_checks import check_estimator\n",
        "\n",
        "check_estimator(MyDecisionTreeRegressor())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "\n",
        "def get_node_name(node):\n",
        "  return f\"{node.depth}{hash(node)}\"\n",
        "\n",
        "def vt(graph, parent_node, child_node, columns_names, is_left):\n",
        "    if (child_node is None):\n",
        "      return\n",
        "\n",
        "    if parent_node is None:\n",
        "      if (child_node.is_terminal):\n",
        "          graph.node(get_node_name(child_node), str(child_node))\n",
        "          return\n",
        "\n",
        "      graph.node(get_node_name(child_node), f\"{columns_names[child_node.column]} >= {child_node.threshold:.2f}\")\n",
        "      vt(graph, child_node, child_node.left, columns_names, True)\n",
        "      vt(graph, child_node, child_node.right, columns_names, False)\n",
        "      return\n",
        "\n",
        "    if (child_node.is_terminal):\n",
        "        graph.node(get_node_name(child_node), str(child_node))\n",
        "        graph.edge(get_node_name(parent_node), get_node_name(child_node), label=f\"{not is_left}\")\n",
        "        return\n",
        "\n",
        "    graph.node(get_node_name(child_node), f\"{columns_names[child_node.column]} >= {child_node.threshold:.2f}\")\n",
        "    graph.edge(get_node_name(parent_node), get_node_name(child_node), label=f\"{not is_left}\")\n",
        "    vt(graph, child_node, child_node.left, columns_names, True)\n",
        "    vt(graph, child_node, child_node.right, columns_names, False)"
      ],
      "metadata": {
        "id": "U1fBAOzdFjhq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qmaMff2dcaP"
      },
      "source": [
        "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
        "\n",
        "Load boston dataset and split it on the train ($75\\%$) and test ($25\\%$). Fit Decision Tree of **depth 1, 3, 5** and make the following plots for every case:\n",
        "\n",
        "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
        "- Fitted model (tree visualization)\n",
        "\n",
        "Compare `MAE` on train and test. Have trees overfitted?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.882171Z",
          "start_time": "2023-11-09T17:46:40.882159Z"
        },
        "id": "XRmkNHNGdcaQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bd96e9b-e39c-4f95-8df9-edd1f209724f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"112pt\" height=\"44pt\"\n viewBox=\"0.00 0.00 111.99 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 107.99,-40 107.99,4 -4,4\"/>\n<!-- 18786639275534 -->\n<g id=\"node1\" class=\"node\">\n<title>18786639275534</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"52\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"52\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 22.91</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fdcc8346e60>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"487pt\" height=\"218pt\"\n viewBox=\"0.00 0.00 486.99 218.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 214)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-214 482.99,-214 482.99,4 -4,4\"/>\n<!-- 18786640294210 -->\n<g id=\"node1\" class=\"node\">\n<title>18786640294210</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"235\" cy=\"-192\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"235\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">RM &gt;= 6.80</text>\n</g>\n<!-- 28786640516508 -->\n<g id=\"node2\" class=\"node\">\n<title>28786640516508</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"156\" cy=\"-105\" rx=\"72.29\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"156\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT &gt;= 14.13</text>\n</g>\n<!-- 18786640294210&#45;&gt;28786640516508 -->\n<g id=\"edge1\" class=\"edge\">\n<title>18786640294210&#45;&gt;28786640516508</title>\n<path fill=\"none\" stroke=\"black\" d=\"M219.76,-174.61C208.04,-161.99 191.65,-144.36 178.42,-130.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"180.82,-127.57 171.45,-122.63 175.69,-132.34 180.82,-127.57\"/>\n<text text-anchor=\"middle\" x=\"215.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 28786640293970 -->\n<g id=\"node5\" class=\"node\">\n<title>28786640293970</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"314\" cy=\"-105\" rx=\"67.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"314\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT &gt;= 3.95</text>\n</g>\n<!-- 18786640294210&#45;&gt;28786640293970 -->\n<g id=\"edge4\" class=\"edge\">\n<title>18786640294210&#45;&gt;28786640293970</title>\n<path fill=\"none\" stroke=\"black\" d=\"M250.23,-174.61C261.96,-161.99 278.34,-144.36 291.57,-130.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"294.3,-132.34 298.54,-122.63 289.17,-127.57 294.3,-132.34\"/>\n<text text-anchor=\"middle\" x=\"293\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 38786640294201 -->\n<g id=\"node3\" class=\"node\">\n<title>38786640294201</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"52\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"52\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 23.03</text>\n</g>\n<!-- 28786640516508&#45;&gt;38786640294201 -->\n<g id=\"edge2\" class=\"edge\">\n<title>28786640516508&#45;&gt;38786640294201</title>\n<path fill=\"none\" stroke=\"black\" d=\"M135.94,-87.61C119.77,-74.39 96.86,-55.67 79.06,-41.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"81.24,-38.38 71.28,-34.76 76.81,-43.8 81.24,-38.38\"/>\n<text text-anchor=\"middle\" x=\"125.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 38786640294216 -->\n<g id=\"node4\" class=\"node\">\n<title>38786640294216</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"174\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"174\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 15.18</text>\n</g>\n<!-- 28786640516508&#45;&gt;38786640294216 -->\n<g id=\"edge3\" class=\"edge\">\n<title>28786640516508&#45;&gt;38786640294216</title>\n<path fill=\"none\" stroke=\"black\" d=\"M159.64,-86.8C162.1,-75.16 165.41,-59.55 168.23,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"171.71,-46.68 170.36,-36.18 164.86,-45.23 171.71,-46.68\"/>\n<text text-anchor=\"middle\" x=\"180\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 38786640294228 -->\n<g id=\"node6\" class=\"node\">\n<title>38786640294228</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"305\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"305\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 42.57</text>\n</g>\n<!-- 28786640293970&#45;&gt;38786640294228 -->\n<g id=\"edge5\" class=\"edge\">\n<title>28786640293970&#45;&gt;38786640294228</title>\n<path fill=\"none\" stroke=\"black\" d=\"M312.17,-86.8C310.94,-75.16 309.29,-59.55 307.88,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"311.35,-45.75 306.81,-36.18 304.39,-46.49 311.35,-45.75\"/>\n<text text-anchor=\"middle\" x=\"325.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 38786640294225 -->\n<g id=\"node7\" class=\"node\">\n<title>38786640294225</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"427\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"427\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 32.76</text>\n</g>\n<!-- 28786640293970&#45;&gt;38786640294225 -->\n<g id=\"edge6\" class=\"edge\">\n<title>28786640293970&#45;&gt;38786640294225</title>\n<path fill=\"none\" stroke=\"black\" d=\"M335.52,-87.81C353.25,-74.47 378.58,-55.42 398.08,-40.75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"400.43,-43.36 406.32,-34.55 396.22,-37.77 400.43,-43.36\"/>\n<text text-anchor=\"middle\" x=\"390\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fdcc8394460>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"1688pt\" height=\"392pt\"\n viewBox=\"0.00 0.00 1687.99 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-388 1683.99,-388 1683.99,4 -4,4\"/>\n<!-- 18786639295792 -->\n<g id=\"node1\" class=\"node\">\n<title>18786639295792</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"915\" cy=\"-366\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"915\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">RM &gt;= 6.80</text>\n</g>\n<!-- 28786640516508 -->\n<g id=\"node2\" class=\"node\">\n<title>28786640516508</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"728\" cy=\"-279\" rx=\"72.29\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"728\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT &gt;= 14.13</text>\n</g>\n<!-- 18786639295792&#45;&gt;28786640516508 -->\n<g id=\"edge1\" class=\"edge\">\n<title>18786639295792&#45;&gt;28786640516508</title>\n<path fill=\"none\" stroke=\"black\" d=\"M884.12,-350.96C853.11,-336.87 805.1,-315.05 770.5,-299.32\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"771.68,-296.01 761.13,-295.06 768.78,-302.38 771.68,-296.01\"/>\n<text text-anchor=\"middle\" x=\"847.5\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 28786632733075 -->\n<g id=\"node17\" class=\"node\">\n<title>28786632733075</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1139\" cy=\"-279\" rx=\"67.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1139\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT &gt;= 3.95</text>\n</g>\n<!-- 18786639295792&#45;&gt;28786632733075 -->\n<g id=\"edge16\" class=\"edge\">\n<title>18786639295792&#45;&gt;28786632733075</title>\n<path fill=\"none\" stroke=\"black\" d=\"M950,-351.72C988.24,-337.21 1049.69,-313.89 1092.09,-297.8\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1093.52,-301 1101.63,-294.18 1091.04,-294.45 1093.52,-301\"/>\n<text text-anchor=\"middle\" x=\"1054\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 38786639466301 -->\n<g id=\"node3\" class=\"node\">\n<title>38786639466301</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"370\" cy=\"-192\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"370\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">DIS &gt;= 1.41</text>\n</g>\n<!-- 28786640516508&#45;&gt;38786639466301 -->\n<g id=\"edge2\" class=\"edge\">\n<title>28786640516508&#45;&gt;38786639466301</title>\n<path fill=\"none\" stroke=\"black\" d=\"M677.46,-266C610.23,-250.04 491.88,-221.94 423.28,-205.65\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"424.07,-202.24 413.53,-203.34 422.45,-209.05 424.07,-202.24\"/>\n<text text-anchor=\"middle\" x=\"585.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 38786639465686 -->\n<g id=\"node10\" class=\"node\">\n<title>38786639465686</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"728\" cy=\"-192\" rx=\"64.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"728\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">CRIM &gt;= 5.87</text>\n</g>\n<!-- 28786640516508&#45;&gt;38786639465686 -->\n<g id=\"edge9\" class=\"edge\">\n<title>28786640516508&#45;&gt;38786639465686</title>\n<path fill=\"none\" stroke=\"black\" d=\"M728,-260.8C728,-249.16 728,-233.55 728,-220.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"731.5,-220.18 728,-210.18 724.5,-220.18 731.5,-220.18\"/>\n<text text-anchor=\"middle\" x=\"741\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 48786632733627 -->\n<g id=\"node4\" class=\"node\">\n<title>48786632733627</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"174\" cy=\"-105\" rx=\"64.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"174\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">CRIM &gt;= 8.27</text>\n</g>\n<!-- 38786639466301&#45;&gt;48786632733627 -->\n<g id=\"edge3\" class=\"edge\">\n<title>38786639466301&#45;&gt;48786632733627</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.07,-177.15C305.11,-162.86 253.48,-140.47 216.96,-124.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218.12,-121.32 207.55,-120.55 215.33,-127.74 218.12,-121.32\"/>\n<text text-anchor=\"middle\" x=\"298.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 48786639466109 -->\n<g id=\"node7\" class=\"node\">\n<title>48786639466109</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"370\" cy=\"-105\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"370\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">DIS &gt;= 3.27</text>\n</g>\n<!-- 38786639466301&#45;&gt;48786639466109 -->\n<g id=\"edge6\" class=\"edge\">\n<title>38786639466301&#45;&gt;48786639466109</title>\n<path fill=\"none\" stroke=\"black\" d=\"M370,-173.8C370,-162.16 370,-146.55 370,-133.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"373.5,-133.18 370,-123.18 366.5,-133.18 373.5,-133.18\"/>\n<text text-anchor=\"middle\" x=\"383\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 58786632644305 -->\n<g id=\"node5\" class=\"node\">\n<title>58786632644305</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"52\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"52\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 50.00</text>\n</g>\n<!-- 48786632733627&#45;&gt;58786632644305 -->\n<g id=\"edge4\" class=\"edge\">\n<title>48786632733627&#45;&gt;58786632644305</title>\n<path fill=\"none\" stroke=\"black\" d=\"M151.05,-88.01C131.64,-74.49 103.66,-55 82.39,-40.18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"84.22,-37.19 74.02,-34.34 80.22,-42.93 84.22,-37.19\"/>\n<text text-anchor=\"middle\" x=\"135.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 58786632644299 -->\n<g id=\"node6\" class=\"node\">\n<title>58786632644299</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"174\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"174\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 38.95</text>\n</g>\n<!-- 48786632733627&#45;&gt;58786632644299 -->\n<g id=\"edge5\" class=\"edge\">\n<title>48786632733627&#45;&gt;58786632644299</title>\n<path fill=\"none\" stroke=\"black\" d=\"M174,-86.8C174,-75.16 174,-59.55 174,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-46.18 174,-36.18 170.5,-46.18 177.5,-46.18\"/>\n<text text-anchor=\"middle\" x=\"187\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 58786632644254 -->\n<g id=\"node8\" class=\"node\">\n<title>58786632644254</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"296\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"296\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 21.62</text>\n</g>\n<!-- 48786639466109&#45;&gt;58786632644254 -->\n<g id=\"edge7\" class=\"edge\">\n<title>48786639466109&#45;&gt;58786632644254</title>\n<path fill=\"none\" stroke=\"black\" d=\"M355.38,-87.21C344.46,-74.67 329.38,-57.35 317.15,-43.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"319.54,-40.71 310.33,-35.47 314.26,-45.31 319.54,-40.71\"/>\n<text text-anchor=\"middle\" x=\"352.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 58786632643990 -->\n<g id=\"node9\" class=\"node\">\n<title>58786632643990</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"418\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"418\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 22.94</text>\n</g>\n<!-- 48786639466109&#45;&gt;58786632643990 -->\n<g id=\"edge8\" class=\"edge\">\n<title>48786639466109&#45;&gt;58786632643990</title>\n<path fill=\"none\" stroke=\"black\" d=\"M379.48,-87.21C386.32,-75.1 395.67,-58.53 403.45,-44.76\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"406.66,-46.19 408.53,-35.76 400.57,-42.74 406.66,-46.19\"/>\n<text text-anchor=\"middle\" x=\"410\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 48786632644008 -->\n<g id=\"node11\" class=\"node\">\n<title>48786632644008</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"658\" cy=\"-105\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"658\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">DIS &gt;= 1.99</text>\n</g>\n<!-- 38786639465686&#45;&gt;48786632644008 -->\n<g id=\"edge10\" class=\"edge\">\n<title>38786639465686&#45;&gt;48786632644008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M714.17,-174.21C703.84,-161.67 689.58,-144.35 678.01,-130.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"680.62,-127.96 671.56,-122.47 675.21,-132.41 680.62,-127.96\"/>\n<text text-anchor=\"middle\" x=\"712.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 48786632643807 -->\n<g id=\"node14\" class=\"node\">\n<title>48786632643807</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"788\" cy=\"-105\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"788\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">DIS &gt;= 1.98</text>\n</g>\n<!-- 38786639465686&#45;&gt;48786632643807 -->\n<g id=\"edge13\" class=\"edge\">\n<title>38786639465686&#45;&gt;48786632643807</title>\n<path fill=\"none\" stroke=\"black\" d=\"M739.85,-174.21C748.56,-161.87 760.54,-144.89 770.37,-130.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"773.26,-132.95 776.17,-122.76 767.54,-128.91 773.26,-132.95\"/>\n<text text-anchor=\"middle\" x=\"775\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 58786632644587 -->\n<g id=\"node12\" class=\"node\">\n<title>58786632644587</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"540\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"540\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 15.14</text>\n</g>\n<!-- 48786632644008&#45;&gt;58786632644587 -->\n<g id=\"edge11\" class=\"edge\">\n<title>48786632644008&#45;&gt;58786632644587</title>\n<path fill=\"none\" stroke=\"black\" d=\"M636.08,-88.21C617.33,-74.71 590.19,-55.16 569.53,-40.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"571.56,-37.42 561.4,-34.42 567.47,-43.1 571.56,-37.42\"/>\n<text text-anchor=\"middle\" x=\"621.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 58786632643825 -->\n<g id=\"node13\" class=\"node\">\n<title>58786632643825</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"662\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"662\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 18.57</text>\n</g>\n<!-- 48786632644008&#45;&gt;58786632643825 -->\n<g id=\"edge12\" class=\"edge\">\n<title>48786632644008&#45;&gt;58786632643825</title>\n<path fill=\"none\" stroke=\"black\" d=\"M658.8,-86.8C659.35,-75.16 660.09,-59.55 660.71,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"664.21,-46.33 661.19,-36.18 657.22,-46 664.21,-46.33\"/>\n<text text-anchor=\"middle\" x=\"674\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 58786632644326 -->\n<g id=\"node15\" class=\"node\">\n<title>58786632644326</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"784\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"784\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 10.52</text>\n</g>\n<!-- 48786632643807&#45;&gt;58786632644326 -->\n<g id=\"edge14\" class=\"edge\">\n<title>48786632643807&#45;&gt;58786632644326</title>\n<path fill=\"none\" stroke=\"black\" d=\"M787.19,-86.8C786.64,-75.16 785.9,-59.55 785.28,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"788.77,-46 784.8,-36.18 781.78,-46.33 788.77,-46\"/>\n<text text-anchor=\"middle\" x=\"801.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 58786632644161 -->\n<g id=\"node16\" class=\"node\">\n<title>58786632644161</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"906\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"906\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 14.81</text>\n</g>\n<!-- 48786632643807&#45;&gt;58786632644161 -->\n<g id=\"edge15\" class=\"edge\">\n<title>48786632643807&#45;&gt;58786632644161</title>\n<path fill=\"none\" stroke=\"black\" d=\"M809.91,-88.21C828.66,-74.71 855.8,-55.16 876.46,-40.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"878.52,-43.1 884.59,-34.42 874.43,-37.42 878.52,-43.1\"/>\n<text text-anchor=\"middle\" x=\"868\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 38786632644278 -->\n<g id=\"node18\" class=\"node\">\n<title>38786632644278</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1139\" cy=\"-192\" rx=\"64.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1139\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">CRIM &gt;= 1.46</text>\n</g>\n<!-- 28786632733075&#45;&gt;38786632644278 -->\n<g id=\"edge17\" class=\"edge\">\n<title>28786632733075&#45;&gt;38786632644278</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1139,-260.8C1139,-249.16 1139,-233.55 1139,-220.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1142.5,-220.18 1139,-210.18 1135.5,-220.18 1142.5,-220.18\"/>\n<text text-anchor=\"middle\" x=\"1153.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 38786632644491 -->\n<g id=\"node23\" class=\"node\">\n<title>38786632644491</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1355\" cy=\"-192\" rx=\"64.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1355\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">CRIM &gt;= 8.25</text>\n</g>\n<!-- 28786632733075&#45;&gt;38786632644491 -->\n<g id=\"edge22\" class=\"edge\">\n<title>28786632733075&#45;&gt;38786632644491</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1175.15,-263.77C1212.1,-249.23 1269.57,-226.62 1309.57,-210.88\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1311.06,-214.05 1319.08,-207.13 1308.5,-207.54 1311.06,-214.05\"/>\n<text text-anchor=\"middle\" x=\"1273\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 48786632644002 -->\n<g id=\"node19\" class=\"node\">\n<title>48786632644002</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1049\" cy=\"-105\" rx=\"67.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1049\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT &gt;= 3.01</text>\n</g>\n<!-- 38786632644278&#45;&gt;48786632644002 -->\n<g id=\"edge18\" class=\"edge\">\n<title>38786632644278&#45;&gt;48786632644002</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1121.64,-174.61C1108.16,-161.88 1089.27,-144.03 1074.13,-129.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1076.28,-126.95 1066.6,-122.63 1071.47,-132.04 1076.28,-126.95\"/>\n<text text-anchor=\"middle\" x=\"1114.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 48786632644377 -->\n<g id=\"node22\" class=\"node\">\n<title>48786632644377</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1187\" cy=\"-105\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1187\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 50.00</text>\n</g>\n<!-- 38786632644278&#45;&gt;48786632644377 -->\n<g id=\"edge21\" class=\"edge\">\n<title>38786632644278&#45;&gt;48786632644377</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1148.71,-173.8C1155.52,-161.74 1164.74,-145.4 1172.43,-131.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1175.59,-133.31 1177.46,-122.89 1169.5,-129.87 1175.59,-133.31\"/>\n<text text-anchor=\"middle\" x=\"1179\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 58786632643588 -->\n<g id=\"node20\" class=\"node\">\n<title>58786632643588</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1028\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1028\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 35.68</text>\n</g>\n<!-- 48786632644002&#45;&gt;58786632643588 -->\n<g id=\"edge19\" class=\"edge\">\n<title>48786632644002&#45;&gt;58786632643588</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1044.75,-86.8C1041.87,-75.16 1038.01,-59.55 1034.72,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1038.04,-45.04 1032.24,-36.18 1031.24,-46.72 1038.04,-45.04\"/>\n<text text-anchor=\"middle\" x=\"1054.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 58786632643612 -->\n<g id=\"node21\" class=\"node\">\n<title>58786632643612</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1150\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1150\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 42.32</text>\n</g>\n<!-- 48786632644002&#45;&gt;58786632643612 -->\n<g id=\"edge20\" class=\"edge\">\n<title>48786632644002&#45;&gt;58786632643612</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1068.47,-87.61C1083.94,-74.59 1105.75,-56.24 1122.92,-41.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1125.53,-44.17 1130.92,-35.05 1121.02,-38.81 1125.53,-44.17\"/>\n<text text-anchor=\"middle\" x=\"1119\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 48786639466277 -->\n<g id=\"node24\" class=\"node\">\n<title>48786639466277</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1355\" cy=\"-105\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1355\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">DIS &gt;= 2.92</text>\n</g>\n<!-- 38786632644491&#45;&gt;48786639466277 -->\n<g id=\"edge23\" class=\"edge\">\n<title>38786632644491&#45;&gt;48786639466277</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1355,-173.8C1355,-162.16 1355,-146.55 1355,-133.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1358.5,-133.18 1355,-123.18 1351.5,-133.18 1358.5,-133.18\"/>\n<text text-anchor=\"middle\" x=\"1369.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 48786639466145 -->\n<g id=\"node27\" class=\"node\">\n<title>48786639466145</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1511\" cy=\"-105\" rx=\"55.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1511\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">DIS &gt;= 2.09</text>\n</g>\n<!-- 38786632644491&#45;&gt;48786639466145 -->\n<g id=\"edge26\" class=\"edge\">\n<title>38786632644491&#45;&gt;48786639466145</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1383.25,-175.61C1408.88,-161.64 1446.83,-140.96 1474.7,-125.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1476.63,-128.71 1483.74,-120.85 1473.28,-122.56 1476.63,-128.71\"/>\n<text text-anchor=\"middle\" x=\"1456\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 58786632644494 -->\n<g id=\"node25\" class=\"node\">\n<title>58786632644494</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1272\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1272\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 37.43</text>\n</g>\n<!-- 48786639466277&#45;&gt;58786632644494 -->\n<g id=\"edge24\" class=\"edge\">\n<title>48786639466277&#45;&gt;58786632644494</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1338.99,-87.61C1326.59,-74.91 1309.23,-57.13 1295.29,-42.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1297.44,-40.05 1287.95,-35.34 1292.43,-44.94 1297.44,-40.05\"/>\n<text text-anchor=\"middle\" x=\"1333.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 58786632643999 -->\n<g id=\"node26\" class=\"node\">\n<title>58786632643999</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1394\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1394\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 32.66</text>\n</g>\n<!-- 48786639466277&#45;&gt;58786632643999 -->\n<g id=\"edge25\" class=\"edge\">\n<title>48786639466277&#45;&gt;58786632643999</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1362.89,-86.8C1368.37,-74.85 1375.77,-58.72 1381.98,-45.18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1385.26,-46.43 1386.25,-35.89 1378.9,-43.51 1385.26,-46.43\"/>\n<text text-anchor=\"middle\" x=\"1390\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 58786639466343 -->\n<g id=\"node28\" class=\"node\">\n<title>58786639466343</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1511\" cy=\"-18\" rx=\"47.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1511\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 9.40</text>\n</g>\n<!-- 48786639466145&#45;&gt;58786639466343 -->\n<g id=\"edge27\" class=\"edge\">\n<title>48786639466145&#45;&gt;58786639466343</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1511,-86.8C1511,-75.16 1511,-59.55 1511,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1514.5,-46.18 1511,-36.18 1507.5,-46.18 1514.5,-46.18\"/>\n<text text-anchor=\"middle\" x=\"1525.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 58786632644188 -->\n<g id=\"node29\" class=\"node\">\n<title>58786632644188</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1628\" cy=\"-18\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1628\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pred: 15.95</text>\n</g>\n<!-- 48786639466145&#45;&gt;58786632644188 -->\n<g id=\"edge28\" class=\"edge\">\n<title>48786639466145&#45;&gt;58786632644188</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1532.73,-88.21C1551.12,-74.85 1577.65,-55.57 1598.05,-40.75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1600.35,-43.41 1606.38,-34.7 1596.23,-37.75 1600.35,-43.41\"/>\n<text text-anchor=\"middle\" x=\"1589\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fdcc942b0a0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MAE:  [6.889290662136855, 3.840700573196252, 2.8314290698845683]\n",
            "Test MAE:  [6.222778551097999, 3.696550581551779, 3.2901301076010525]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\n",
        "    filepath_or_buffer=\"https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/sklearn/datasets/data/boston_house_prices.csv\",\n",
        "    skiprows=1\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop(\"MEDV\", axis=1)\n",
        "y = df.MEDV\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=42)\n",
        "\n",
        "from sklearn.tree import plot_tree\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import graphviz\n",
        "\n",
        "mae_train = []\n",
        "mae_test = []\n",
        "for depth in range(1, 6, 2):\n",
        "    clf = MyDecisionTreeRegressor(max_depth=depth)\n",
        "    clf.fit(X_train, y_train)\n",
        "    prediction = clf.predict(X_test)\n",
        "    for feature, thresholds in clf.split_columns_.items():\n",
        "        plt.scatter(X_train[X.columns[feature]], y_train)\n",
        "        for threshold in thresholds:\n",
        "            plt.axvline(x=threshold, color=\"r\", linestyle='--')\n",
        "        plt.xlabel(X.columns[feature])\n",
        "        plt.ylabel(\"MEDV\")\n",
        "        plt.title(f\"Depth = {depth}\")\n",
        "        plt.show()\n",
        "    mae_train.append(mean_absolute_error(y_train, clf.predict(X_train)))\n",
        "    mae_test.append(mean_absolute_error(y_test, prediction))\n",
        "    graph = graphviz.Digraph()\n",
        "    vt(graph, None, clf.tree_, X.columns, False)\n",
        "    display(graph)\n",
        "print(\"Train MAE: \", mae_train)\n",
        "print(\"Test MAE: \", mae_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train MAE:  [6.889290662136855, 3.5124363697281904, 2.3731022950108898]  \n",
        "Test MAE:  [6.222778551097999, 3.533031771028621, 2.8890456467977614]\n",
        "\n",
        "Деревья с большей глубиной сильнее переобучаются. Такой вывод можно сделать по полученным значениям MAE. Деревья переобучены тогда, когда на train значение MAE сильно ниже значения на test"
      ],
      "metadata": {
        "id": "p2BE4zGefuVJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc1CGwp_dcaT"
      },
      "source": [
        "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
        "\n",
        "Keep working with boston dataset.\n",
        "- Use `GridSearchCV` to find the best hyperparameters among [`max_depth`, `min_samples_leaf`] on 5-Fold cross-validation\n",
        "- Train the model with the best set of hyperparameters on the whole train dataset.\n",
        "- Report `MAE` on test dataset and hyperparameters of the best estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.884202Z",
          "start_time": "2023-11-09T17:46:40.884125Z"
        },
        "id": "HLysssN4dcaU"
      },
      "outputs": [],
      "source": [
        "# https://github.com/darkydash/ml_hse/blob/main/week07/Seminar_07_Decision_trees_autumn23%20(1).ipynb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "clf = MyDecisionTreeRegressor()\n",
        "\n",
        "parameters = {'max_depth': [1, 3, 5, 7, 10],\n",
        "              'min_samples_leaf': [1, 5, 10, 50]}\n",
        "\n",
        "gs = GridSearchCV(clf,\n",
        "                  parameters,\n",
        "                  scoring=\"neg_mean_absolute_error\",\n",
        "                  cv=5)\n",
        "\n",
        "result = gs.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.best_params_)\n",
        "clf = MyDecisionTreeRegressor(**result.best_params_)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "yVAjKaGVTf0s",
        "outputId": "0425e338-92bf-4eb3-d673-99a90c842e28"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 7, 'min_samples_leaf': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyDecisionTreeRegressor(max_depth=7)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MyDecisionTreeRegressor(max_depth=7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MyDecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>MyDecisionTreeRegressor(max_depth=7)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"MAE value on test with params {result.best_params_} is: {mean_absolute_error(y_test, clf.predict(X_test))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHpgKm59gIEf",
        "outputId": "0a011444-cc87-4729-c582-8bcf4120ab9c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE value on test with params {'max_depth': 7, 'min_samples_leaf': 1} is: 3.025972979181438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9RbQeS6dcaW"
      },
      "source": [
        "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
        "\n",
        "Recall definition of bias and variance:\n",
        "$$\n",
        "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
        "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
        "$$\n",
        "\n",
        "We wil now use the following algorithm to estimate bias and variance:\n",
        "\n",
        "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
        "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
        "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
        "4. For a given *object* $n$:\n",
        "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
        "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
        "5. Average bias^2 and variance over all the points\n",
        "    \n",
        "**Implement `get_bias_variance` function, using the algorithm above**\n",
        "\n",
        "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.887222Z",
          "start_time": "2023-11-09T17:46:40.887195Z"
        },
        "id": "mmsrYTIadcaX"
      },
      "outputs": [],
      "source": [
        "def get_bias_variance(estimator, x, y, n_iter):\n",
        "    \"\"\"\n",
        "    Calculate bias and variance of the `estimator`.\n",
        "    Using a given dataset and bootstrap with `n_iter` samples.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray, shape (n_samples, n_features)\n",
        "        The input samples.\n",
        "    y : ndarray, shape (n_samples, n_features)\n",
        "        The input samples.\n",
        "    n_iter: int\n",
        "        Number of samples in\n",
        "    Returns\n",
        "    -------\n",
        "    bias2 : float,\n",
        "        Estiamted squared bias\n",
        "    variance : float,\n",
        "        Estiamted variance\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_samples = x.shape[0]\n",
        "    n_features = x.shape[1]\n",
        "\n",
        "    y_pred = np.empty((n_samples, n_features))\n",
        "    y_pred[:] = np.nan\n",
        "\n",
        "    for bootstarp_iter in range(n_iter):\n",
        "        idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        X_i = x[idx]\n",
        "        mask = np.ones(n_samples, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        Z_i = x[mask]\n",
        "        estimator.fit(X_i, y[idx])\n",
        "        y_pred[mask, bootstarp_iter] = estimator.predict(Z_i)\n",
        "\n",
        "    bias_sq = np.nanmean((y - np.nanmean(y_pred, axis=1))**2)\n",
        "    variance = np.nanmean(np.nanvar(y_pred, axis=1))\n",
        "    return bias_sq, variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.888721Z",
          "start_time": "2023-11-09T17:46:40.888706Z"
        },
        "id": "OkKiZ4UxdcaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d0e6b7-3897-4858-cadc-cafa5044c964"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22.22743813720527, 9.772064006609709)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# Test\n",
        "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
        "\n",
        "get_bias_variance(estimator, X_train.values, y_train.values, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M0koec1dcab"
      },
      "source": [
        "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
        "\n",
        "Compute bias and variance for the trees with different min_samples_split. Plot how bias and variance change as min_samples_split increases.\n",
        "\n",
        "Comment on what you observe, how does your result correspond to theory?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.889695Z",
          "start_time": "2023-11-09T17:46:40.889683Z"
        },
        "id": "uXLcecwTdcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "a36de24c-dbab-4768-c5b5-687462df38dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "min_samples_split_values = [i for i in range(5, 105, 5)]\n",
        "bias_values = []\n",
        "var_values = []\n",
        "for min_samples_split in min_samples_split_values:\n",
        "    clf = MyDecisionTreeRegressor(max_depth=8, min_samples_split=min_samples_split)\n",
        "    bias, variance = get_bias_variance(clf, X_train.values, y_train.values, 10)\n",
        "    bias_values.append(bias)\n",
        "    var_values.append(variance)\n",
        "\n",
        "plt.plot(min_samples_split_values, bias_values, label=\"bias\")\n",
        "plt.plot(min_samples_split_values, var_values, label=\"variance\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Beut0hjZdcae"
      },
      "source": [
        "С увеличением `min_samples_split` глубина дерева тоже будет уменьшаться, так как минимальное количество объектов в промежуточных узлах должно быть всё больше и больше. По этой причине деревья недообучаются и смещение возрастает. Разброс же уменьшается, так как модели становятся менее разнообразными."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl0NwbuGdcae"
      },
      "source": [
        "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
        "\n",
        "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance.\n",
        "\n",
        "Answer the following questions:\n",
        " - How bagging should affect bias and variance in theory?\n",
        " - How bias and variance change (if they change) compared to an individual tree in you experiments?\n",
        " - Do your results align with the theory? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.891228Z",
          "start_time": "2023-11-09T17:46:40.891211Z"
        },
        "id": "3hNHOH3xdcaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5f0cf6-78b7-4204-93ff-04258f6c9c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Squared Bias: 21.576676845480193\n",
            "Ensemble Variance: 3.2950237957987354\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "ensemble = BaggingRegressor(estimator=MyDecisionTreeRegressor(max_depth=8, min_samples_split=15),\n",
        "                     n_estimators=10, random_state=42)\n",
        "\n",
        "bias, variance = get_bias_variance(ensemble, X_train.values, y_train.values, 10)\n",
        "\n",
        "\n",
        "print(f\"Ensemble Squared Bias: {bias}\")\n",
        "print(f\"Ensemble Variance: {variance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9PvWP9Ddcag"
      },
      "source": [
        "В теории использование бэггинга должно уменьшать разброс, так как увеличивается разнообразие моделей, что позволяет компенсировать ошибки при усреднении ответов. При этом значимого влияния на смещения быть не должно  \n",
        "\n",
        "Смещение уменьшилось несильно, разброс значимо сократился  \n",
        "\n",
        "Результаты согласуются с теорией, так как разброс сильно сокартился, а смещение изменилось несильно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SGyS1Vldcag"
      },
      "source": [
        "# Part 2. More Ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEQdQ9PCdcah"
      },
      "source": [
        "In this part we will be working with [Billionaires Statistics Dataset](https://www.kaggle.com/datasets/nelgiriyewithana/billionaires-statistics-dataset) to solve a classification task."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/darkydash/ml_hse/main/hw4/Billionaires%20Statistics%20Dataset.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCirSgDHTAfd",
        "outputId": "b4c281a9-c8ba-4255-ef19-c8f13e456042"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-26 14:22:39--  https://raw.githubusercontent.com/darkydash/ml_hse/main/hw4/Billionaires%20Statistics%20Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 677819 (662K) [text/plain]\n",
            "Saving to: ‘Billionaires Statistics Dataset.csv.14’\n",
            "\n",
            "\r          Billionai   0%[                    ]       0  --.-KB/s               \rBillionaires Statis 100%[===================>] 661.93K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-11-26 14:22:39 (89.6 MB/s) - ‘Billionaires Statistics Dataset.csv.14’ saved [677819/677819]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:47:35.163040Z",
          "start_time": "2023-11-09T17:47:35.075668Z"
        },
        "id": "YqVpHrmpdcai",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "87ea33bb-9180-4421-9da8-d10af99718a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rank  finalWorth               category                personName   age  \\\n",
              "0     1      211000       Fashion & Retail  Bernard Arnault & family  74.0   \n",
              "1     2      180000             Automotive                 Elon Musk  51.0   \n",
              "2     3      114000             Technology                Jeff Bezos  59.0   \n",
              "3     4      107000             Technology             Larry Ellison  78.0   \n",
              "4     5      106000  Finance & Investments            Warren Buffett  92.0   \n",
              "\n",
              "         country    city              source             industries  \\\n",
              "0         France   Paris                LVMH       Fashion & Retail   \n",
              "1  United States  Austin       Tesla, SpaceX             Automotive   \n",
              "2  United States  Medina              Amazon             Technology   \n",
              "3  United States   Lanai              Oracle             Technology   \n",
              "4  United States   Omaha  Berkshire Hathaway  Finance & Investments   \n",
              "\n",
              "  countryOfCitizenship  ... cpi_change_country           gdp_country  \\\n",
              "0               France  ...                1.1   $2,715,518,274,227    \n",
              "1        United States  ...                7.5  $21,427,700,000,000    \n",
              "2        United States  ...                7.5  $21,427,700,000,000    \n",
              "3        United States  ...                7.5  $21,427,700,000,000    \n",
              "4        United States  ...                7.5  $21,427,700,000,000    \n",
              "\n",
              "  gross_tertiary_education_enrollment  \\\n",
              "0                                65.6   \n",
              "1                                88.2   \n",
              "2                                88.2   \n",
              "3                                88.2   \n",
              "4                                88.2   \n",
              "\n",
              "  gross_primary_education_enrollment_country life_expectancy_country  \\\n",
              "0                                      102.5                    82.5   \n",
              "1                                      101.8                    78.5   \n",
              "2                                      101.8                    78.5   \n",
              "3                                      101.8                    78.5   \n",
              "4                                      101.8                    78.5   \n",
              "\n",
              "  tax_revenue_country_country total_tax_rate_country population_country  \\\n",
              "0                        24.2                   60.7         67059887.0   \n",
              "1                         9.6                   36.6        328239523.0   \n",
              "2                         9.6                   36.6        328239523.0   \n",
              "3                         9.6                   36.6        328239523.0   \n",
              "4                         9.6                   36.6        328239523.0   \n",
              "\n",
              "  latitude_country longitude_country  \n",
              "0        46.227638          2.213749  \n",
              "1        37.090240        -95.712891  \n",
              "2        37.090240        -95.712891  \n",
              "3        37.090240        -95.712891  \n",
              "4        37.090240        -95.712891  \n",
              "\n",
              "[5 rows x 34 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8b20c0a-7651-4eb1-8a3a-12bcbe9392fa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rank</th>\n",
              "      <th>finalWorth</th>\n",
              "      <th>category</th>\n",
              "      <th>personName</th>\n",
              "      <th>age</th>\n",
              "      <th>country</th>\n",
              "      <th>city</th>\n",
              "      <th>source</th>\n",
              "      <th>industries</th>\n",
              "      <th>countryOfCitizenship</th>\n",
              "      <th>...</th>\n",
              "      <th>cpi_change_country</th>\n",
              "      <th>gdp_country</th>\n",
              "      <th>gross_tertiary_education_enrollment</th>\n",
              "      <th>gross_primary_education_enrollment_country</th>\n",
              "      <th>life_expectancy_country</th>\n",
              "      <th>tax_revenue_country_country</th>\n",
              "      <th>total_tax_rate_country</th>\n",
              "      <th>population_country</th>\n",
              "      <th>latitude_country</th>\n",
              "      <th>longitude_country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>211000</td>\n",
              "      <td>Fashion &amp; Retail</td>\n",
              "      <td>Bernard Arnault &amp; family</td>\n",
              "      <td>74.0</td>\n",
              "      <td>France</td>\n",
              "      <td>Paris</td>\n",
              "      <td>LVMH</td>\n",
              "      <td>Fashion &amp; Retail</td>\n",
              "      <td>France</td>\n",
              "      <td>...</td>\n",
              "      <td>1.1</td>\n",
              "      <td>$2,715,518,274,227</td>\n",
              "      <td>65.6</td>\n",
              "      <td>102.5</td>\n",
              "      <td>82.5</td>\n",
              "      <td>24.2</td>\n",
              "      <td>60.7</td>\n",
              "      <td>67059887.0</td>\n",
              "      <td>46.227638</td>\n",
              "      <td>2.213749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>180000</td>\n",
              "      <td>Automotive</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>51.0</td>\n",
              "      <td>United States</td>\n",
              "      <td>Austin</td>\n",
              "      <td>Tesla, SpaceX</td>\n",
              "      <td>Automotive</td>\n",
              "      <td>United States</td>\n",
              "      <td>...</td>\n",
              "      <td>7.5</td>\n",
              "      <td>$21,427,700,000,000</td>\n",
              "      <td>88.2</td>\n",
              "      <td>101.8</td>\n",
              "      <td>78.5</td>\n",
              "      <td>9.6</td>\n",
              "      <td>36.6</td>\n",
              "      <td>328239523.0</td>\n",
              "      <td>37.090240</td>\n",
              "      <td>-95.712891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>114000</td>\n",
              "      <td>Technology</td>\n",
              "      <td>Jeff Bezos</td>\n",
              "      <td>59.0</td>\n",
              "      <td>United States</td>\n",
              "      <td>Medina</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>Technology</td>\n",
              "      <td>United States</td>\n",
              "      <td>...</td>\n",
              "      <td>7.5</td>\n",
              "      <td>$21,427,700,000,000</td>\n",
              "      <td>88.2</td>\n",
              "      <td>101.8</td>\n",
              "      <td>78.5</td>\n",
              "      <td>9.6</td>\n",
              "      <td>36.6</td>\n",
              "      <td>328239523.0</td>\n",
              "      <td>37.090240</td>\n",
              "      <td>-95.712891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>107000</td>\n",
              "      <td>Technology</td>\n",
              "      <td>Larry Ellison</td>\n",
              "      <td>78.0</td>\n",
              "      <td>United States</td>\n",
              "      <td>Lanai</td>\n",
              "      <td>Oracle</td>\n",
              "      <td>Technology</td>\n",
              "      <td>United States</td>\n",
              "      <td>...</td>\n",
              "      <td>7.5</td>\n",
              "      <td>$21,427,700,000,000</td>\n",
              "      <td>88.2</td>\n",
              "      <td>101.8</td>\n",
              "      <td>78.5</td>\n",
              "      <td>9.6</td>\n",
              "      <td>36.6</td>\n",
              "      <td>328239523.0</td>\n",
              "      <td>37.090240</td>\n",
              "      <td>-95.712891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>106000</td>\n",
              "      <td>Finance &amp; Investments</td>\n",
              "      <td>Warren Buffett</td>\n",
              "      <td>92.0</td>\n",
              "      <td>United States</td>\n",
              "      <td>Omaha</td>\n",
              "      <td>Berkshire Hathaway</td>\n",
              "      <td>Finance &amp; Investments</td>\n",
              "      <td>United States</td>\n",
              "      <td>...</td>\n",
              "      <td>7.5</td>\n",
              "      <td>$21,427,700,000,000</td>\n",
              "      <td>88.2</td>\n",
              "      <td>101.8</td>\n",
              "      <td>78.5</td>\n",
              "      <td>9.6</td>\n",
              "      <td>36.6</td>\n",
              "      <td>328239523.0</td>\n",
              "      <td>37.090240</td>\n",
              "      <td>-95.712891</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 34 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8b20c0a-7651-4eb1-8a3a-12bcbe9392fa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8b20c0a-7651-4eb1-8a3a-12bcbe9392fa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8b20c0a-7651-4eb1-8a3a-12bcbe9392fa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6f63ff8-3e04-4d34-96d6-24ca2f16d48c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6f63ff8-3e04-4d34-96d6-24ca2f16d48c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6f63ff8-3e04-4d34-96d6-24ca2f16d48c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv('Billionaires Statistics Dataset.csv')\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['selfMade'])\n",
        "X = df.drop('selfMade', axis=1)\n",
        "X.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM4OT8Pmdcaj"
      },
      "source": [
        "### Task 1 <a id=\"task2_1\"></a> (1 point)\n",
        "\n",
        "Let's start with data preprocessing.\n",
        "\n",
        "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice.\n",
        "1. Split dataset into train and test\n",
        "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
        "    - Categorical: Fill missing values and apply one-hot-encoding (if there are many unique values in a column, you can group them by meaning)\n",
        "    - Numeric: Fill missing values\n",
        "    \n",
        "Use `ColumnTranformer` to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
        "\n",
        "```\n",
        "ColumnTransformer([\n",
        "    ('name1', transform1, column_names1),\n",
        "    ('name2', transform2, column_names2)\n",
        "])\n",
        "```\n",
        "\n",
        "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer.\n",
        "    \n",
        "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
        "\n",
        "```\n",
        "double_tranform = make_pipeline(\n",
        "                        transform_1,\n",
        "                        transform_2\n",
        "                        )\n",
        "```\n",
        "\n",
        "P.S. Choose your favourite way to fill missing values.\n",
        "\n",
        "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6W2EWz-Us_D",
        "outputId": "87af1cae-21f8-4c63-a77a-6dc16fe9bf7a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2640 entries, 0 to 2639\n",
            "Data columns (total 34 columns):\n",
            " #   Column                                      Non-Null Count  Dtype  \n",
            "---  ------                                      --------------  -----  \n",
            " 0   rank                                        2640 non-null   int64  \n",
            " 1   finalWorth                                  2640 non-null   int64  \n",
            " 2   category                                    2640 non-null   object \n",
            " 3   personName                                  2640 non-null   object \n",
            " 4   age                                         2575 non-null   float64\n",
            " 5   country                                     2602 non-null   object \n",
            " 6   city                                        2568 non-null   object \n",
            " 7   source                                      2640 non-null   object \n",
            " 8   industries                                  2640 non-null   object \n",
            " 9   countryOfCitizenship                        2640 non-null   object \n",
            " 10  organization                                325 non-null    object \n",
            " 11  status                                      2640 non-null   object \n",
            " 12  gender                                      2640 non-null   object \n",
            " 13  birthDate                                   2564 non-null   object \n",
            " 14  lastName                                    2640 non-null   object \n",
            " 15  firstName                                   2637 non-null   object \n",
            " 16  title                                       339 non-null    object \n",
            " 17  date                                        2640 non-null   object \n",
            " 18  state                                       753 non-null    object \n",
            " 19  residenceStateRegion                        747 non-null    object \n",
            " 20  birthYear                                   2564 non-null   float64\n",
            " 21  birthMonth                                  2564 non-null   float64\n",
            " 22  birthDay                                    2564 non-null   float64\n",
            " 23  cpi_country                                 2456 non-null   float64\n",
            " 24  cpi_change_country                          2456 non-null   float64\n",
            " 25  gdp_country                                 2476 non-null   object \n",
            " 26  gross_tertiary_education_enrollment         2458 non-null   float64\n",
            " 27  gross_primary_education_enrollment_country  2459 non-null   float64\n",
            " 28  life_expectancy_country                     2458 non-null   float64\n",
            " 29  tax_revenue_country_country                 2457 non-null   float64\n",
            " 30  total_tax_rate_country                      2458 non-null   float64\n",
            " 31  population_country                          2476 non-null   float64\n",
            " 32  latitude_country                            2476 non-null   float64\n",
            " 33  longitude_country                           2476 non-null   float64\n",
            "dtypes: float64(14), int64(2), object(18)\n",
            "memory usage: 701.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используя полученную таблицу `info` и сведения к колонкам отберем признаки:\n",
        "- признаки `personName`, `lastName`, `firstName` не несут информации полезной для предсказания.\n",
        "- признаки `organization`, `title`, `state`, `residenceStateRegion` имеют много пропущенных значений, поэтому имеет смысл просто выкинуть их\n",
        "- `birthDate`, `birthMonth`, `birthYear`, `birthDay` дублируют признак `age`\n",
        "- `date` &mdash; время сбора данных, что не несет полезной информации"
      ],
      "metadata": {
        "id": "ua2ENuf8dR7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.drop(columns=[\"personName\", \"lastName\", \"firstName\", \"organization\", \"title\", \"state\", \"residenceStateRegion\", \"birthDate\", \"birthMonth\", \"birthYear\", \"birthDay\", \"date\"])"
      ],
      "metadata": {
        "id": "ZAtE9WPugu0j"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"gdp_country\"] = X[\"gdp_country\"].str[1:]\n",
        "X[\"gdp_country\"] = X[\"gdp_country\"].str.replace(\",\", \"\").astype(\"float64\")\n",
        "X.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyjWP2_yfNPa",
        "outputId": "d1207a48-b190-4320-fc65-7e5aaecaee7d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2640 entries, 0 to 2639\n",
            "Data columns (total 22 columns):\n",
            " #   Column                                      Non-Null Count  Dtype  \n",
            "---  ------                                      --------------  -----  \n",
            " 0   rank                                        2640 non-null   int64  \n",
            " 1   finalWorth                                  2640 non-null   int64  \n",
            " 2   category                                    2640 non-null   object \n",
            " 3   age                                         2575 non-null   float64\n",
            " 4   country                                     2602 non-null   object \n",
            " 5   city                                        2568 non-null   object \n",
            " 6   source                                      2640 non-null   object \n",
            " 7   industries                                  2640 non-null   object \n",
            " 8   countryOfCitizenship                        2640 non-null   object \n",
            " 9   status                                      2640 non-null   object \n",
            " 10  gender                                      2640 non-null   object \n",
            " 11  cpi_country                                 2456 non-null   float64\n",
            " 12  cpi_change_country                          2456 non-null   float64\n",
            " 13  gdp_country                                 2476 non-null   float64\n",
            " 14  gross_tertiary_education_enrollment         2458 non-null   float64\n",
            " 15  gross_primary_education_enrollment_country  2459 non-null   float64\n",
            " 16  life_expectancy_country                     2458 non-null   float64\n",
            " 17  tax_revenue_country_country                 2457 non-null   float64\n",
            " 18  total_tax_rate_country                      2458 non-null   float64\n",
            " 19  population_country                          2476 non-null   float64\n",
            " 20  latitude_country                            2476 non-null   float64\n",
            " 21  longitude_country                           2476 non-null   float64\n",
            "dtypes: float64(12), int64(2), object(8)\n",
            "memory usage: 453.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = X.industries == X.category\n",
        "print(r.value_counts())\n",
        "X = X.drop(columns=[\"industries\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxjZtFWTiraB",
        "outputId": "1c5cda3d-224d-46f0-b789-4af38dc4572e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True    2640\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEm9Gv8hiwCC",
        "outputId": "45941d2c-fe23-49d7-9910-6d3b7668b368"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2640 entries, 0 to 2639\n",
            "Data columns (total 21 columns):\n",
            " #   Column                                      Non-Null Count  Dtype  \n",
            "---  ------                                      --------------  -----  \n",
            " 0   rank                                        2640 non-null   int64  \n",
            " 1   finalWorth                                  2640 non-null   int64  \n",
            " 2   category                                    2640 non-null   object \n",
            " 3   age                                         2575 non-null   float64\n",
            " 4   country                                     2602 non-null   object \n",
            " 5   city                                        2568 non-null   object \n",
            " 6   source                                      2640 non-null   object \n",
            " 7   countryOfCitizenship                        2640 non-null   object \n",
            " 8   status                                      2640 non-null   object \n",
            " 9   gender                                      2640 non-null   object \n",
            " 10  cpi_country                                 2456 non-null   float64\n",
            " 11  cpi_change_country                          2456 non-null   float64\n",
            " 12  gdp_country                                 2476 non-null   float64\n",
            " 13  gross_tertiary_education_enrollment         2458 non-null   float64\n",
            " 14  gross_primary_education_enrollment_country  2459 non-null   float64\n",
            " 15  life_expectancy_country                     2458 non-null   float64\n",
            " 16  tax_revenue_country_country                 2457 non-null   float64\n",
            " 17  total_tax_rate_country                      2458 non-null   float64\n",
            " 18  population_country                          2476 non-null   float64\n",
            " 19  latitude_country                            2476 non-null   float64\n",
            " 20  longitude_country                           2476 non-null   float64\n",
            "dtypes: float64(12), int64(2), object(7)\n",
            "memory usage: 433.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.country.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6GOfvXakGtp",
        "outputId": "f4320669-e024-4387-e0cf-c6ed10b9d222"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "United States           754\n",
              "China                   523\n",
              "India                   157\n",
              "Germany                 102\n",
              "United Kingdom           82\n",
              "                       ... \n",
              "Portugal                  1\n",
              "Georgia                   1\n",
              "Eswatini (Swaziland)      1\n",
              "Uzbekistan                1\n",
              "Armenia                   1\n",
              "Name: country, Length: 78, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Akf08mOddcal"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "categorical_pipline = make_pipeline(OneHotEncoder(drop='first'))\n",
        "numeric_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"))\n",
        "categorical = [col for col in X.columns if X[col].dtype == np.dtype(\"object\")]\n",
        "numeric = [col for col in X.columns if X[col].dtype != np.dtype(\"object\")]\n",
        "# define column_transformer\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('categorical', categorical_pipline, categorical),\n",
        "    ('numeric', numeric_pipeline, numeric)\n",
        "])\n",
        "\n",
        "# Transform the data\n",
        "column_transformer.fit(X)\n",
        "X_train = column_transformer.transform(X_train)\n",
        "X_test = column_transformer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv_eN9Fndcam"
      },
      "source": [
        "### Task 2 <a id=\"task2_2\"></a> (0.7 points)\n",
        "\n",
        "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
        "    \n",
        "* Choose one classification metric and justify your choice .\n",
        "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
        "* Which model has the best performance? Which models overfit or underfit?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['selfMade'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22ybnaUCxnYL",
        "outputId": "558f6c22-8426-4a3b-f609-e9adef5a5da1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     1812\n",
              "False     828\n",
              "Name: selfMade, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Классы несбалансированы, поэтому будем использовать метрику, учитывающую такое распределние: AP (average_precision)"
      ],
      "metadata": {
        "id": "iS_GhCS3A0qZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "4vQH1_XEdcao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29af46ec-39e2-45e8-9c6e-11122fd06f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting scores: [0.9212842  0.9295363  0.9366401  0.92690353 0.90465402] | Gradient Boosting mean score: 0.9238036277712812\n",
            "Random Forest scores: [0.92456754 0.92629346 0.91911301 0.92933138 0.92611656] | Random Forest mean score: 0.9250843905810514\n",
            "Decision Tree scores: [0.81169793 0.80428679 0.8028373  0.79452158 0.77963244] | Decision Tree mean score: 0.7985952072774484\n",
            "SVM scores: [0.72303368 0.80471658 0.72494215 0.71569542 0.62573247] | SVM mean score: 0.7188240592785277\n",
            "Logitics Regression scores: [0.72927097 0.74601203 0.74213418 0.73779517 0.70832712] | Logitics Regression mean score: 0.7327078939150755\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import average_precision_score, make_scorer\n",
        "\n",
        "scorer = make_scorer(average_precision_score, greater_is_better=True,\n",
        "                             needs_threshold=True)\n",
        "cv_skf = StratifiedKFold(n_splits=5)\n",
        "random_state = 42\n",
        "clfs = {\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=random_state),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=random_state),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=random_state),\n",
        "    \"SVM\": SVC(probability=True, random_state=random_state),\n",
        "    \"Logitics Regression\": LogisticRegression(random_state=random_state)\n",
        "}\n",
        "\n",
        "for clf_name, clf in clfs.items():\n",
        "    scores = cross_val_score(clf, X_train, y_train, cv = cv_skf, n_jobs=-1, scoring=scorer)\n",
        "    print(f\"{clf_name} scores: {scores} | {clf_name} mean score: {np.mean(scores)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXpO-0ymdcar"
      },
      "source": [
        "Лучший результат показала модель случайного леса. По результатам остальных моделей можно сделать вывод, что `SVM` и `Logit` не дообучились. Это могло вызвать отсутствие подобора гиперпараметров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143RaKvodcat"
      },
      "source": [
        "### Task 3 <a id=\"task2_3\"></a> (0.5 points)\n",
        "\n",
        "More Gradient Boosting. You will have to take one of the three popular boosting implementations (xgboost, lightgbm, catboost). Select hyperparameters (number of trees, learning rate, depth) on cross-validation and compare with the methods from the previous task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "QemjOMNqdcau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b8322a-8a50-4ab1-ba7c-185bd9dda86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000373 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000560 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001260 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002295 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000377 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000496 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000412 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000494 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000576 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000576 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000506 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000490 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000493 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000577 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000513 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000458 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000471 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000571 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000576 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000510 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000551 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000490 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000660 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000511 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000634 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000477 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000525 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000415 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000428 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000489 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000494 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001378 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000551 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001263 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000384 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000663 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000394 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000937 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000492 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000388 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000377 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000510 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000431 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000821 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000393 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000658 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000849 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000560 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000496 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000368 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000560 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000527 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001430 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000512 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000634 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000599 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000551 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000534 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000513 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000433 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000402 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 898\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000506 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 917\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 94\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000393 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 919\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 881\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 88\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1092, number of negative: 492\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 920\n",
            "[LightGBM] [Info] Number of data points in the train set: 1584, number of used features: 96\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Number of positive: 1365, number of negative: 615\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 987\n",
            "[LightGBM] [Info] Number of data points in the train set: 1980, number of used features: 107\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.689394 -> initscore=0.797287\n",
            "[LightGBM] [Info] Start training from score 0.797287\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "parameters = {\n",
        "    \"n_estimators\": np.arange(1, 300, 25),\n",
        "    \"learning_rate\": [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
        "    \"max_depth\": np.arange(1, 100, 10)\n",
        "}\n",
        "clf = LGBMClassifier()\n",
        "cv_skf = StratifiedKFold(n_splits=5)\n",
        "gsclf = GridSearchCV(clf,\n",
        "                  parameters,\n",
        "                  scoring=scorer,\n",
        "                  cv=cv_skf)\n",
        "result = gsclf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyAaevKYAQWy",
        "outputId": "03a24ce4-a880-4b6c-f73a-e605c7e59bdf"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9223292180542891"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VthnSsDU--A",
        "outputId": "3244c5f3-2c69-4419-8a3a-2c398bcdb093"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.1, 'max_depth': 31, 'n_estimators': 76}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель показала сравнимый результат с лучшей из прошлого задания."
      ],
      "metadata": {
        "id": "B-13z9KXreeQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVxgQZmLdca1"
      },
      "source": [
        "### Task 4 <a id=\"task2_4\"></a> (0.7 points)\n",
        "\n",
        "Now let's train more fancy ensembles:\n",
        "\n",
        "* Bagging with decision trees as base estimators\n",
        "* Bagging with gradient boosting (with large amount of trees, >100) as base estimators\n",
        "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)\n",
        "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
        "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
        "\n",
        "\n",
        "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
        "\n",
        "Answer the questions:\n",
        "* Which model has the best performance?\n",
        "* Does bagging reduce overfiting of the gradient boosting with large amount of trees?\n",
        "* What is the difference between voting and staking?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "S5rd6vzNdca2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"n_estimators\": [10, 20, 30],\n",
        "    \"max_samples\": [20, 30, 40],\n",
        "    \"max_features\": [30, 40, 50]\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(BaggingClassifier(),\n",
        "                  parameters,\n",
        "                  cv=StratifiedKFold(),\n",
        "                  scoring=scorer)\n",
        "gs.fit(X_train, y_train)\n",
        "print(f\"Average Precision on train: {scorer(gs, X_train, y_train)}\")\n",
        "print(f\"Average Precision on test: {scorer(gs, X_test, y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXd4iS71YNhs",
        "outputId": "b14dc424-ddb9-4ac6-c83e-eb3e6e682a5e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision on train: 0.8518386234175634\n",
            "Average Precision on test: 0.8501081199805681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"n_estimators\": [10, 20, 30],\n",
        "    \"max_samples\": [20, 30, 40],\n",
        "    \"max_features\": [30, 40, 50]\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(BaggingClassifier(GradientBoostingClassifier(n_estimators=200)),\n",
        "                  parameters,\n",
        "                  cv=StratifiedKFold(),\n",
        "                  scoring=scorer)\n",
        "gs.fit(X_train, y_train)\n",
        "print(f\"Average Precision on train: {scorer(gs, X_train, y_train)}\")\n",
        "print(f\"Average Precision on test: {scorer(gs, X_test, y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoLA5Z7DZjsi",
        "outputId": "7ed54c7b-4351-4a01-8d35-fdfcff80478f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision on train: 0.8431434696665867\n",
            "Average Precision on test: 0.8460773979545585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "M65t7FAKZ4yf"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = {\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=random_state),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=random_state),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=random_state),\n",
        "    \"SVM\": SVC(probability=True, random_state=random_state),\n",
        "    \"Logitics Regression\": LogisticRegression(random_state=random_state)\n",
        "}"
      ],
      "metadata": {
        "id": "DlTW_udhfm11"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
        "clf = VotingClassifier(estimators=[(x, y) for x, y in clfs.items()], voting=\"soft\")\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"Average Precision on train: {scorer(clf, X_train, y_train)}\")\n",
        "print(f\"Average Precision on test: {scorer(clf, X_test, y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd6jF62_aAxx",
        "outputId": "f760b8b5-1346-4363-e1dd-51d0a6d8fb5d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision on train: 1.0\n",
            "Average Precision on test: 0.9256792821033173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = {\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=random_state),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=random_state),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=random_state),\n",
        "    \"SVM\": SVC(probability=True, random_state=random_state),\n",
        "    \"Logitics Regression\": LogisticRegression(random_state=random_state)\n",
        "}"
      ],
      "metadata": {
        "id": "ZZFUGrzCfn2O"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "clf = StackingClassifier(estimators=[(x, y) for x, y in clfs.items()], final_estimator=LogisticRegression())\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"Average Precision on train: {scorer(clf, X_train, y_train)}\")\n",
        "print(f\"Average Precision on test: {scorer(clf, X_test, y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGvbeLxDdAbA",
        "outputId": "a6cc46a9-9870-4d70-cd62-80af98b9351a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision on train: 0.9998301682816576\n",
            "Average Precision on test: 0.9457278685525738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = {\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=random_state),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=random_state),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=random_state),\n",
        "    \"SVM\": SVC(probability=True, random_state=random_state),\n",
        "    \"Logitics Regression\": LogisticRegression(random_state=random_state)\n",
        "}"
      ],
      "metadata": {
        "id": "KV6MItlTfpeL"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = StackingClassifier(estimators=[(x, y) for x, y in clfs.items()], final_estimator=GradientBoostingClassifier())\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"Average Precision on train: {scorer(clf, X_train, y_train)}\")\n",
        "print(f\"Average Precision on test: {scorer(clf, X_test, y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvNK0VOMdUXo",
        "outputId": "fcfe352f-e851-427c-e915-548c208a08c9"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision on train: 0.9983252998241633\n",
            "Average Precision on test: 0.9384544822700207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhrQ2bTkdca3"
      },
      "source": [
        "\n",
        "Лучший результат показал `Stacking Classifier with Logistic Regression as a final model`\n",
        "\n",
        "Бэггинг помог сократить переобучение у градиентного спуска с большим количеством деревьев. Такой вывод можно сделать на основе того, что значение метрики AP примерно равно на тестовых и тренировочных данных. И меньше, чем в предыдущих заданиях. Это объясняется тем, что разные деревья видели разные части датасета и поэтому впоследствии начали компенсировать ошибки друг друга.\n",
        "\n",
        "В voting мы обучаем несколько моделей независимо, а в качестве предсказания сначала собираем все предикты моделей и выдаем ответ на основе большинства \"голосов\" за некоторый класс.  \n",
        "\n",
        "В стэкинге мы обучаем несколько моделей, собираем их результаты и на этих полученных результатах обучаем ещё одну \"финальную\" модель.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnhdeHNFdca4"
      },
      "source": [
        "### Task 5 <a id=\"task2_5\"></a> (0.1 points)\n",
        "\n",
        "Report the test score for the best model, that you were able to train."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для всех моделей ранее были посчитаны значения метрики на тестовых данных. Так, лучшей моделью оказался `Stacking Classifier with Logistic Regression as a final model`"
      ],
      "metadata": {
        "id": "8CvCYUuXtNfU"
      }
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}